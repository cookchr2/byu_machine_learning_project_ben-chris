{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the source code for our project! I apologize for the mess; this was coded over the course of the semester and a good chunk of last semester. As such, I don't have a straighforward list of code that will take you through everything that I did. Rather, this code will give you a summary of what happened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorry, but I've chosen to omit most of this part because it's long, messy, and not super relevant. In addition, it was all coded in STATA. Here's a general overview of what the code did:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create the 1900-1910 crosswalk from the pid-1900 and pid-1910 crosswalks\n",
    "* Merge the crosswalk onto both the 1900 and 1910 censuses\n",
    "* Extract Features from the matches\n",
    "* Go back and merge again onto censuses to get false matches. (With better planning this could have been avoided, but what's done is done)\n",
    "* Extract Features from false matches\n",
    "* Grab potential matches\n",
    "\n",
    "False and potential matches were obtained as described in the presentation. Namely we binned by birthyear (5 year birth bins), first letter of first name, first letter of last name, sex, birthplace, mother's birthplace and father's birthplace. False matches were comparisons between 1900 and 1910 within a bin where one record was already matched to another in the true set. Potential matches were all comparisons between 1900 and 1910 within bins where neither record belonged to a match in the true set. Very large bins were omitted from potential matches for tractability. \n",
    "\n",
    "For completeness, the do files are included in the folder dofiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is relevant for transparency, here's the code I used to extract features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "**THIS IS STATA CODE DO NOT RUN IN NOTEBOOK\n",
    "cd R:\\JoePriceResearch\\record_linking\\projects\\deep_learning\\census\\lg\n",
    "set more off, perm\n",
    "local states = \"Alabama Arkansas California Colorado Connecticut Delaware Florida Georgia Idaho Illinois Indiana Iowa Kansas Kentucky Louisiana Maine Maryland Massachusetts Michigan Minnesota Mississippi Missouri Montana Nebraska Nevada New_Hampshire New_Jersey New_York_part1 New_York_part2 North_Carolina North_Dakota Ohio Oregon Pennsylvania_part1 Pennsylvania_part2 Rhode_Island South_Carolina South_Dakota Tennessee Texas Utah Vermont Virginia Washington West_Virginia Wisconsin Wyoming\"\n",
    "set seed 21\n",
    "\n",
    "*Loop through each state \n",
    "\n",
    "foreach state in `states' {\n",
    "\tforeach name in `state' `state'1 {\n",
    "        *open file\n",
    "\t\tuse `name', clear\n",
    "\t\t\n",
    "\t\tkeep if true == 0 //just so that we don't have to calculate them twice\n",
    "\t\t\n",
    "        *get name frequencies\n",
    "\t\tgen pr_name_gn = poi_name_gn1900\n",
    "\t\tmerge m:1 pr_name_gn using R:\\JoePriceResearch\\record_linking\\data\\census_1910\\data\\obj\\names_crosswalk.dta\n",
    "\t\tdrop if _merge == 2\n",
    "\t\trename tot poi_name_freq1900\n",
    "\t\treplace pr_name_gn = poi_name_gn1910\n",
    "\t\tdrop _merge\n",
    "\t\tmerge m:1 pr_name_gn using R:\\JoePriceResearch\\record_linking\\data\\census_1910\\data\\obj\\names_crosswalk.dta\n",
    "\t\tdrop if _merge == 2\n",
    "\t\tdrop _merge\n",
    "\t\trename tot poi_name_freq1910\n",
    "\t\tdrop pr_name_gn\n",
    "\n",
    "\n",
    "\t\tgen names = poi_name_surn1900\n",
    "\t\tmerge m:1 names using R:\\JoePriceResearch\\record_linking\\data\\census_1910\\data\\obj\\last_names_crosswalk.dta\n",
    "\t\tdrop if _merge == 2\n",
    "\t\trename _0 poi_surn_freq1900\n",
    "\t\treplace names = poi_name_surn1910\n",
    "\t\tdrop _merge\n",
    "\t\tmerge m:1 names using R:\\JoePriceResearch\\record_linking\\data\\census_1910\\data\\obj\\last_names_crosswalk.dta\n",
    "\t\tdrop if _merge == 2\n",
    "\t\tdrop _merge\n",
    "\t\trename _0 poi_surn_freq1910\n",
    "\t\tdrop names\n",
    "\n",
    "\t\t** create some features based on the given name and lastname\n",
    "\t\tfor X in any 1900 1910: rename poi_name_gnX gnameX\n",
    "\t\tfor X in any 1900 1910: rename poi_name_surnX lnameX\n",
    "        \n",
    "        **Create jarowinkler, initial, matching soundex features\n",
    "\t\tforeach Z in gname lname {\n",
    "\t\t\tforeach X in 1900 1910  {\n",
    "\t\t\t\treplace `Z'`X' = lower(trim(`Z'`X')) \n",
    "\t\t\t}\n",
    "\t\t\tforeach X in 1900 1910 {\n",
    "\t\t\t\treplace `Z'`X' = trim(`Z'`X') \n",
    "\t\t\t}\n",
    "\t\t\tgen `Z'_length = abs(length(`Z'1900) - length(`Z'1910))\n",
    "\t\t\tjarowinkler `Z'1900 `Z'1910, generate(`Z'_jaro) \n",
    "\t\t\tgen `Z'_same = `Z'1900==`Z'1910\n",
    "\t\t\tgen `Z'_firstl = substr(`Z'1900,1,1)==substr(`Z'1910,1,1)\n",
    "\t\t\tgen `Z'_lastl = substr(`Z'1900,-1,1)==substr(`Z'1910,-1,1)\n",
    "\t\t\tgen `Z'_soundex = soundex(`Z'1900) == soundex(`Z'1910)\n",
    "\t\t\t}\n",
    "\n",
    "\t\tgen name_same = gname1900==gname1910 & lname1900==lname1910\n",
    "\n",
    "        *create birthyear features\n",
    "\t\tgen diff_birthyear = abs(poi_birthyear1900 - poi_birthyear1910)\n",
    "\t\tgen birthyr_abs1 = 1 == diff_birthyear\n",
    "\t\tgen birthyr_abs2 = 2 == diff_birthyear\n",
    "\t\tgen birthyr_abs3 = 3 == diff_birthyear\n",
    "\t\tgen birthyr_same = 0 == diff_birthyear\n",
    "\n",
    "\t\t** check about middle initial (convert thses lines into a single regular expression)\n",
    "\t\tforeach X in 1900 1910 {\n",
    "\t\t\tsplit gname`X'\n",
    "\t\t}\n",
    "\t\tforeach X in 1900 1910 {\n",
    "\t\t\tgen midinit`X' = substr(gname`X'2,1,1) \n",
    "\t\t}\n",
    "\t\tgen midinit_match = midinit1900 == midinit1910\n",
    "\t\treplace midinit_match = 0 if midinit1900 == \"\"\n",
    "    \n",
    "        *create sex_same feature\n",
    "\t\tgen sex_same = 0\n",
    "\t\treplace sex_same = 1 if poi_sex1900 == poi_sex1910\n",
    "\n",
    "\t\t*namelength\n",
    "\n",
    "\t\tgen namelength1900 = strlen(gname1900 + lname1900)\n",
    "\t\tgen namelength1910 = strlen(gname1910 + lname1910)\n",
    "\n",
    "\t\t*Distance // created in later code: not included\n",
    "\n",
    "\t\t*Same State\n",
    "\t\tgen same_state = 0\n",
    "\t\treplace state1910 = strtrim(state1910)\n",
    "\t\treplace state1900 = strtrim(state1900)\n",
    "\t\treplace county1910 = strtrim(county1910)\n",
    "\t\treplace county1900 = strtrim(county1900)\n",
    "\t\treplace same_state = 1 if state1900 == state1910\n",
    "\t\treplace same_state = 0 if state1900 == \"\"\n",
    "\n",
    "\t\t*Same county\n",
    "\t\tgen same_county = 0\n",
    "\t\treplace same_county = 1 if county1900 == county1910\n",
    "\t\treplace same_county = 0 if county1900 == \"\"\n",
    "\n",
    "\n",
    "\t\t*Same Township\n",
    "\n",
    "\t\tgen same_township = 0\n",
    "\t\treplace same_township = 1 if township1900 == township1910\n",
    "\t\treplace same_township = 0 if township1900 == \"\"\n",
    "\n",
    "\t\tjarowinkler township1900 township1910, generate(township_jaro) \n",
    "\n",
    "\n",
    "\t\t*Same race\n",
    "\n",
    "\t\tgen same_race = 0\n",
    "\t\treplace same_race = 1 if poi_race1900 == poi_race1910 & poi_race1900 != \"\"\n",
    "\n",
    "\t\t*Race dist\n",
    "\n",
    "\t\tgen race_dist = 2\n",
    "\t\treplace race_dist = 0 if poi_race1900 == poi_race1910 & poi_race1900 != \"\"\n",
    "\t\treplace race_dist = 1 if poi_race1900 == \"Black\" & poi_race1910 == \"Mulatto\"\n",
    "\t\treplace race_dist = 1 if poi_race1900 == \"Mulatto\" & poi_race1910 == \"Black\"\n",
    "\t\treplace race_dist = 1 if poi_race1900 == \"White\" & poi_race1910 == \"Mulatto\"\n",
    "\t\treplace race_dist = 1 if poi_race1900 == \"Mulatto\" & poi_race1910 == \"White\"\n",
    "\n",
    "\t\t* same bpl\n",
    "\t\tgen same_bpl = 0\n",
    "\t\treplace same_bpl = 1 if poi_birthplace1900 == poi_birthplace1910 & poi_birthplace1900 != \"\"\n",
    "\n",
    "\t\t*dist bpl\n",
    "\n",
    "\t\t*mpbl fbpl, (dist)\n",
    "\t\tgen same_mbpl = 0\n",
    "\t\treplace same_mbpl = 1 if poi_mbpl1900 == poi_mbpl1910 & poi_mbpl1900 != \"\"\n",
    "\n",
    "\t\tgen same_fbpl = 0\n",
    "\t\treplace same_fbpl = 1 if poi_fbpl1900 == poi_fbpl1910 & poi_fbpl1900 != \"\"\n",
    "\n",
    "\n",
    "\t\t*imm yr\n",
    "\n",
    "\t\tgen same_imm_yr = 0\n",
    "\t\tdestring poi_imm_yr1900, replace\n",
    "\t\tdestring poi_imm_yr1910, replace\n",
    "\t\treplace same_imm_yr = 1 if poi_imm_yr1900 == poi_imm_yr1910 & poi_imm_yr1900 != .\n",
    "\n",
    "\t\t*Uncommon and common names\n",
    "\t\tgen uncommon_name1900 = 0\n",
    "\t\treplace uncommon_name1900 = 1 if poi_name_freq1900 <= 20\n",
    "\n",
    "\t\tgen uncommon_name1910 = 0\n",
    "\t\treplace uncommon_name1910 = 1 if poi_name_freq1910 <= 20\n",
    "\n",
    "\t\tgen common_name1900 = 0\n",
    "\t\treplace common_name1900 = 1 if poi_name_freq1900 >= 1000000\n",
    "\n",
    "\t\tgen common_name1910 = 0\n",
    "\t\treplace common_name1910 = 1 if poi_name_freq1910 >= 1000000\n",
    "\n",
    "\t\tgen uncommon_surn1900 = 0\n",
    "\t\treplace uncommon_surn1900 = 1 if poi_surn_freq1900 <= 20\n",
    "\n",
    "\t\tgen uncommon_surn1910 = 0\n",
    "\t\treplace uncommon_surn1910 = 1 if poi_surn_freq1910 <= 20\n",
    "\n",
    "\t\tgen common_surn1900 = 0\n",
    "\t\treplace common_surn1900 = 1 if poi_surn_freq1900 >= 1000000\n",
    "\n",
    "\t\tgen common_surn1910 = 0\n",
    "\t\treplace common_surn1910 = 1 if poi_surn_freq1910 >= 1000000\n",
    "\n",
    "\t\t* #of names\n",
    "\n",
    "\t\tsplit lname1900\n",
    "\t\t\n",
    "\t\tif \"`r(nvars)'\" == \"1\" {\n",
    "\t\t\tgen lname19002 = \"\"\n",
    "\t\t}\n",
    "\t\t\n",
    "\t\tsplit lname1910\n",
    "\t\tif \"`r(nvars)'\" == \"1\" {\n",
    "\t\t\tgen lname19102 = \"\"\n",
    "\t\t}\t\t\n",
    "\t\t\n",
    "\n",
    "\t\tgen num_names1900 = 0\n",
    "\t\treplace num_names1900 = num_names1900 + 1 if gname19001 != \"\"\n",
    "\t\treplace num_names1900 = num_names1900 + 1 if gname19002 != \"\"\n",
    "\n",
    "\t\treplace num_names1900 = num_names1900 + 1 if lname19001 != \"\"\n",
    "\t\treplace num_names1900 = num_names1900 + 1 if lname19002 != \"\"\n",
    "\n",
    "\t\tgen num_names1910 = 0\n",
    "\t\treplace num_names1910 = num_names1910 + 1 if gname19101 != \"\"\n",
    "\t\treplace num_names1910 = num_names1910 + 1 if gname19102 != \"\"\n",
    "\n",
    "\t\treplace num_names1910 = num_names1910 + 1 if lname19101 != \"\"\n",
    "\t\treplace num_names1910 = num_names1910 + 1 if lname19102 != \"\"\n",
    "\n",
    "\n",
    "\t\t* Create max and avg jw features.\n",
    "\n",
    "\t\tjarowinkler gname19001 gname19101, generate(n1) \n",
    "\t\tjarowinkler gname19001 gname19102, generate(n2) \n",
    "\t\tjarowinkler gname19001 lname19101, generate(n4) \n",
    "\t\tjarowinkler gname19001 lname19102, generate(n5) \n",
    "\n",
    "\t\tjarowinkler gname19002 gname19101, generate(n7) \n",
    "\t\tjarowinkler gname19002 gname19102, generate(n8) \n",
    "\t\tjarowinkler gname19002 lname19101, generate(n10) \n",
    "\t\tjarowinkler gname19002 lname19102, generate(n11) \n",
    "\n",
    "\n",
    "\t\tjarowinkler lname19001 gname19101, generate(n19) \n",
    "\t\tjarowinkler lname19001 gname19102, generate(n20) \n",
    "\t\tjarowinkler lname19001 lname19101, generate(n22) \n",
    "\t\tjarowinkler lname19001 lname19102, generate(n23) \n",
    "\t\t\n",
    "\t\t\n",
    "\t\tjarowinkler lname19002 gname19101, generate(n25) \n",
    "\t\tjarowinkler lname19002 gname19102, generate(n26) \n",
    "\t\tjarowinkler lname19002 lname19101, generate(n28) \n",
    "\t\tjarowinkler lname19002 lname19102, generate(n29) \n",
    "\n",
    "\n",
    "\t\treplace n1 = . if gname19001 == \"\" | gname19101 == \"\" \n",
    "\t\treplace n2 = . if gname19001 == \"\" | gname19102 == \"\" \n",
    "\t\treplace n4 = . if gname19001 == \"\" |  lname19101 == \"\"  \n",
    "\t\treplace n5 = . if gname19001 == \"\" |  lname19102 == \"\"  \n",
    "\n",
    "\t\treplace n7 = . if gname19002 == \"\" |  gname19101 == \"\" \n",
    "\t\treplace n8 = . if gname19002 == \"\" |  gname19102 == \"\" \n",
    "\t\treplace n10 = . if gname19002 == \"\" |  lname19101 == \"\"  \n",
    "\t\treplace n11 = . if gname19002 == \"\" |  lname19102 == \"\"  \n",
    "\n",
    "\n",
    "\t\treplace n19 = . if lname19001 == \"\" |  gname19101 == \"\" \n",
    "\t\treplace n20 = . if lname19001 == \"\" |  gname19102 == \"\"  \n",
    "\t\treplace n22 = . if lname19001 == \"\" |  lname19101 == \"\"  \n",
    "\t\treplace n23 = . if lname19001 == \"\" |  lname19102 == \"\"  \n",
    "\n",
    "\t\treplace n25 = . if lname19002 == \"\" |  gname19101 == \"\"  \n",
    "\t\treplace n26 = . if lname19002 == \"\" |  gname19102 == \"\"  \n",
    "\t\treplace n28 = . if lname19002 == \"\" |  lname19101 == \"\"  \n",
    "\t\treplace n29 = . if lname19002 == \"\" |  lname19102 == \"\"  \n",
    "\n",
    "\t\tegen k1 = rowmax(n1 n2 n4 n5)\n",
    "\t\tegen k2 = rowmax(n7-n11)\n",
    "\t\tegen k4 = rowmax(n19-n23)\n",
    "\t\tegen k5 = rowmax(n25-n29)\n",
    "\n",
    "\t\tegen j1 = rowmax(n1 n7 n19 n25)\n",
    "\t\tegen j2 = rowmax(n2 n8 n20 n26)\n",
    "\t\tegen j4 = rowmax(n4 n10 n22 n28)\n",
    "\t\tegen j5 = rowmax(n5 n11 n23 n29)\n",
    "\n",
    "\t\tegen max_jw_gn = rowmax(k1 k2 j1 j2)\n",
    "\t\tegen max_jw_surn = rowmax(k4 k5 j4 j5)\n",
    "\n",
    "\t\t* Avg JW\n",
    "\n",
    "\t\tegen avg_jw_dim1 = rowmean(k1 k2 k4 k5)\n",
    "\t\tegen avg_jw_dim2 = rowmean(j1 j2 j4 j5)\n",
    "\n",
    "\t\tsave `name'_scored, replace\n",
    "\n",
    "\t}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though there were various iterations, I've decided to only include examples of my code for the Random Forest and Neural Net. This is because the OLS and SVM models are easily adapted from the Random Forest code by replacing where appropriate. It was implemented in Spyder, so please forgive the lack of markdown areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Jan 12 11:34:37 2018\n",
    "\n",
    "@author: cookchr2\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#importing the necessary packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy\n",
    "import time\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "\n",
    "\n",
    "#fix random seed for reproducibility\n",
    "seed = 21\n",
    "\n",
    "#Set our random seed\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "#This is the directory \n",
    "os.chdir('R:/JoePriceResearch/record_linking/projects/deep_learning/census')\n",
    "\n",
    "#Open Data\n",
    "X = pd.read_stata('virpilot_machine_take2dl.dta')\n",
    "\n",
    "#Drop a random sample of false matches\n",
    "X['sort'] = pd.Series(numpy.random.uniform(0.0,1.0,X['true'].count()))\n",
    "X = X[(X.sort > 0.6) | (X.true == 1)]\n",
    "X = X.drop(['sort'],axis=1)\n",
    "\n",
    "#Create Y variable\n",
    "Y = X['true']\n",
    "\n",
    "#Split into train and test\n",
    "X, Xtest, Y, Ytest = model_selection.train_test_split(X, Y, test_size=0.20, random_state=21)\n",
    "\n",
    "#I had a hard time getting the native confusion matrix to work. This was my solution:\n",
    "#split it myself. So we have seperate true and false testing sets\n",
    "Ytestf = Ytest[(Ytest == 0)]\n",
    "Xtestf = Xtest[(Xtest.true == 0)]\n",
    "Ytest = Ytest[(Ytest == 1)]\n",
    "Xtest = Xtest[(Xtest.true == 1)]\n",
    "\n",
    "#Remove Y from the X matrix before training\n",
    "X = X.drop(['true'], axis=1)\n",
    "Xtest = Xtest.drop(['true'], axis=1)\n",
    "Xtestf = Xtestf.drop(['true'], axis=1)\n",
    "\n",
    "\n",
    "# Declare Model \n",
    "#Note: these are the best parameters I found; I used the commented out gridsearch to get them, over several iterations\n",
    "rf_gs = ensemble.RandomForestClassifier(n_estimators = 1000, max_features = 0.8, max_depth = 15, n_jobs = -1)\n",
    "#rf_gs = model_selection.GridSearchCV(rf, {'n_estimators' : [1000], 'max_features':[None], 'max_depth':[20], 'n_jobs':[-1] })\n",
    "\n",
    "\n",
    "\n",
    "#fit  models\n",
    "\n",
    "#get the start time\n",
    "t0 = time.time()\n",
    "print('starting')\n",
    "rf_gs.fit(X.values, Y.values)\n",
    "t1 = time.time()\n",
    "\n",
    "#print in sample accuracy; good to check overfitting\n",
    "in_rf_acc = rf_gs.score(X.values, Y.values)\n",
    "print('In sample predictions: ' + str(in_rf_acc))\n",
    "\n",
    "#Print testing true and false accuracy\n",
    "rf_acct = rf_gs.score(Xtest.values, Ytest.values)\n",
    "rf_accf = rf_gs.score(Xtestf.values, Ytestf.values)\n",
    "\n",
    "print('True: ' + str(rf_acct))\n",
    "print('False: ' + str(rf_accf))\n",
    "\n",
    "#print the time and optionally the best parameters used\n",
    "total = t1-t0\n",
    "print(total/60)\n",
    "#print(\"Best Params: {}\".format(rf_gs.best_params_))\n",
    "\n",
    "\n",
    "#Save model\n",
    "pickle.dump(rf_gs, open('R:/JoePriceResearch/record_linking/projects/deep_learning/census/rf5.sav', 'wb'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Jan 12 11:34:37 2018\n",
    "\n",
    "@author: cookchr2\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#importing the necessary packages\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "\n",
    "#get the start time\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "#fix random seed for reproducibility\n",
    "seed = 21\n",
    "\n",
    "#This let's numpy use our random seed\n",
    "np.random.seed(seed)\n",
    "\n",
    "#This is the directory \n",
    "os.chdir('R:/JoePriceResearch/record_linking/projects/deep_learning/census')\n",
    "\n",
    "#read in data\n",
    "df = pd.read_stata('virpilot_machine_take2dl.dta')\n",
    "\n",
    "#Sort data\n",
    "df['sort'] = pd.Series(np.random.uniform(size=df.shape[0]))\n",
    "\n",
    "#Creat Y matrix\n",
    "Y = pd.DataFrame()\n",
    "Y['true'] = df['true']\n",
    "Y['false'] = 1 - df['true']\n",
    "Y['sort'] = df['sort']\n",
    "\n",
    "#split train and test\n",
    "X, Xtest, Y, Ytest = train_test_split(df,Y,test_size = 0.2, random_state=2222)\n",
    "\n",
    "#Drop a random sample of false matches\n",
    "Y = Y[(Y.sort >= 0.5) | (Y.true == 1)]\n",
    "X = X[(X.sort >= 0.5) | (X.true == 1)]\n",
    "\n",
    "#get rid of sort variable\n",
    "X = X.drop(['true','sort'], axis=1)\n",
    "Y = Y.drop(['sort'], axis=1)\n",
    "Ytest = Ytest.drop(['sort'], axis=1)\n",
    "Xtest = Xtest.drop(['sort'], axis=1)\n",
    "\n",
    "#Split into true and false\n",
    "Ytestt = Ytest[(Ytest.true == 1)]\n",
    "Xtestt = Xtest[(Xtest.true == 1)]\n",
    "Ytestf = Ytest[(Ytest.true == 0)]\n",
    "Xtestf = Xtest[(Xtest.true == 0)]\n",
    "\n",
    "#remove true variable before training\n",
    "Xtestt = Xtestt.drop(['true'], axis=1)\n",
    "Xtestf = Xtestf.drop(['true'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create a model--Note that this is just the last model I tried. Dropout helped fix overfitting, but\n",
    "#I don't think the BatchNormalization did much\n",
    "model = Sequential([Dense(50, input_dim=44, activation=\"relu\", kernel_initializer='he_normal'),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(0.5),\n",
    "                    Dense(25, activation=\"relu\", kernel_initializer='he_normal'),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(0.5),\n",
    "                    Dense(12, activation=\"relu\", kernel_initializer='he_normal'),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(0.5),\n",
    "                    Dense(6, activation=\"relu\", kernel_initializer='he_normal'),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(0.5),\n",
    "                    Dense(3, activation=\"relu\", kernel_initializer='he_normal'),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(0.3),\n",
    "                    Dense(2, activation=\"softmax\", kernel_initializer='he_normal')\n",
    "                    \n",
    "        ])\n",
    "\n",
    "\n",
    "#Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#fit: again, I tried A LOT of values here\n",
    "model.fit(X.values, Y.values, epochs=4, batch_size=16,validation_split=0.1)\n",
    "\n",
    "\n",
    "#save the recently trained model\n",
    "\n",
    "model.save('R:/JoePriceResearch/record_linking/projects/deep_learning/census/model21.h5')\n",
    "\n",
    "#evalute\n",
    "\n",
    "#Get a score for true\n",
    "scores = model.evaluate(Xtestt.values, Ytestt.values)\n",
    "print('True %s: %.f%%' % (model.metrics_names[1], scores[1]*1000))\n",
    "\n",
    "#and a score for false\n",
    "scores = model.evaluate(Xtestf.values, Ytestf.values)\n",
    "print('False %s: %.f%%' % (model.metrics_names[1], scores[1]*1000))\n",
    "\n",
    "#print how long it took\n",
    "t1 = time.time()\n",
    "\n",
    "total = t1-t0\n",
    "print(total/60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the visualization in our final product involved manually recording the output of the various models we ran. However, we did grab the features importances graph from our random forest, so here's the code we used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import pandas as pd\n",
    "import pickle \n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.chdir('R:/JoePriceResearch/record_linking/projects/deep_learning/census/lg')\n",
    "\n",
    "#Open finished model\n",
    "rf_gs = pickle.load(open('R:/JoePriceResearch/record_linking/projects/deep_learning/census/rf5.sav', 'rb'))\n",
    "\n",
    "#open data\n",
    "X = pd.read_stata('data_all_wd.dta')\n",
    "X = X.drop(['true'], axis=1)\n",
    "\n",
    "#create importances\n",
    "feature_imp = sorted(list(zip(list(X), rf_gs.feature_importances_)), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "#plot\n",
    "pd.Series([x[1] for x in feature_imp], index=[x[0] for x in feature_imp]).plot(kind='bar',color='b')\n",
    "\n",
    "#save plot\n",
    "plt.savefig('importances.png', bbox_inches='tight',dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
